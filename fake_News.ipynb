{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-Akhlaghi246/PYTHON/blob/main/fake_News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "id": "UiBXnj160TDz",
        "outputId": "e990e76b-22ca-496f-ee12-00fd702d7d21"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2-780005940.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2-780005940.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install hazm\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm\n",
        "!pip install gensim\n",
        "!pip install python-bidi\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install --upgrade numpy pandas\n",
        "!pip install numpy==1.26.4 pandas==2.1.4\n",
        "!pip install --no-binary :all: pandas\n",
        "!pip install --upgrade pandas pandasai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "PWSok8BpznLu",
        "outputId": "9b996d3a-a877-488e-8f80-fde819b4cd40"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'arabic_reshaper'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3265440552.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbidi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0marabic_reshaper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'arabic_reshaper'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from hazm import Normalizer, word_tokenize, Stemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from gensim.models import Word2Vec\n",
        "from bidi.algorithm import get_display\n",
        "import arabic_reshaper\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class PersianFakeNewsDetector:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = None\n",
        "        self.model = None\n",
        "        self.word2vec_model = None\n",
        "        self.normalizer = Normalizer()\n",
        "        self.stemmer = Stemmer()\n",
        "\n",
        "    def load_dataset(self, file_path):\n",
        "        \"\"\"بارگذاری دیتاست از فایل CSV\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            print(f\"دیتاست با موفقیت بارگذاری شد. تعداد رکوردها: {len(df)}\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"خطا در بارگذاری دیتاست: {e}\")\n",
        "            return None\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"پیش‌پردازش متن فارسی\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # نرمال‌سازی متن\n",
        "        text = self.normalizer.normalize(text)\n",
        "\n",
        "        # حذف علائم نگارشی و اعداد\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        text = re.sub(r'\\d+', ' ', text)\n",
        "\n",
        "        # توکن‌سازی و ریشه‌یابی\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [self.stemmer.stem(token) for token in tokens if token.strip() != '']\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def prepare_dataset(self, df, text_column='text', label_column='label'):\n",
        "        \"\"\"آماده‌سازی دیتاست برای آموزش\"\"\"\n",
        "        # پیش‌پردازش متون\n",
        "        df['processed_text'] = df[text_column].apply(self.preprocess_text)\n",
        "\n",
        "        # تبدیل برچسب‌ها به عدد\n",
        "        df['label'] = df[label_column].map({'real': 0, 'fake': 1})\n",
        "\n",
        "        return df\n",
        "\n",
        "    def train_tfidf_model(self, df):\n",
        "        \"\"\"آموزش مدل با روش TF-IDF\"\"\"\n",
        "        # تقسیم داده به آموزش و آزمون\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            df['processed_text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "        # تبدیل متن به بردار با TF-IDF\n",
        "        self.vectorizer = TfidfVectorizer(max_features=5000)\n",
        "        X_train_tfidf = self.vectorizer.fit_transform(X_train)\n",
        "        X_test_tfidf = self.vectorizer.transform(X_test)\n",
        "\n",
        "        # آموزش مدل رگرسیون لجستیک\n",
        "        self.model = LogisticRegression(max_iter=1000)\n",
        "        self.model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "        # ارزیابی مدل\n",
        "        y_pred = self.model.predict(X_test_tfidf)\n",
        "        print(\"نتایج ارزیابی مدل TF-IDF:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(f\"دقت مدل: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "    def train_word2vec_model(self, df):\n",
        "        \"\"\"آموزش مدل Word2Vec برای متن‌های فارسی\"\"\"\n",
        "        sentences = [word_tokenize(text) for text in df['processed_text']]\n",
        "        self.word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "        # تبدیل متون به بردار با میانگین بردارهای کلمات\n",
        "        def text_to_vector(text):\n",
        "            words = word_tokenize(text)\n",
        "            word_vectors = [self.word2vec_model.wv[word] for word in words if word in self.word2vec_model.wv]\n",
        "            if len(word_vectors) == 0:\n",
        "                return np.zeros(100)\n",
        "            return np.mean(word_vectors, axis=0)\n",
        "\n",
        "        df['word2vec'] = df['processed_text'].apply(text_to_vector)\n",
        "\n",
        "        # تقسیم داده به آموزش و آزمون\n",
        "        X = np.stack(df['word2vec'].values)\n",
        "        y = df['label'].values\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # آموزش مدل\n",
        "        self.model = LogisticRegression(max_iter=1000)\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "        # ارزیابی مدل\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        print(\"\\nنتایج ارزیابی مدل Word2Vec:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(f\"دقت مدل: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "    def predict_news(self, news_text):\n",
        "        \"\"\"پیش‌بینی واقعی یا جعلی بودن خبر\"\"\"\n",
        "        if self.model is None or self.vectorizer is None:\n",
        "            print(\"لطفا ابتدا مدل را آموزش دهید\")\n",
        "            return None\n",
        "\n",
        "        # پیش‌پردازش متن ورودی\n",
        "        processed_text = self.preprocess_text(news_text)\n",
        "\n",
        "        if self.word2vec_model:\n",
        "            # اگر از Word2Vec استفاده شده\n",
        "            vector = np.mean([self.word2vec_model.wv[word] for word in word_tokenize(processed_text)\n",
        "                            if word in self.word2vec_model.wv] or [np.zeros(100)], axis=0)\n",
        "            vector = vector.reshape(1, -1)\n",
        "        else:\n",
        "            # اگر از TF-IDF استفاده شده\n",
        "            vector = self.vectorizer.transform([processed_text])\n",
        "\n",
        "        # پیش‌بینی\n",
        "        prediction = self.model.predict(vector)[0]\n",
        "        probability = self.model.predict_proba(vector)[0]\n",
        "\n",
        "        result = {\n",
        "            'text': news_text,\n",
        "            'processed_text': processed_text,\n",
        "            'prediction': 'جعلی' if prediction == 1 else 'واقعی',\n",
        "            'fake_probability': f\"{probability[1]:.2%}\",\n",
        "            'real_probability': f\"{probability[0]:.2%}\"\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def display_result(self, result):\n",
        "        \"\"\"نمایش زیبای نتیجه\"\"\"\n",
        "        if not result:\n",
        "            return\n",
        "\n",
        "        reshaped_text = arabic_reshaper.reshape(result['text'])\n",
        "        bidi_text = get_display(reshaped_text)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"متن اصلی:\\n{bidi_text}\")\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "        print(f\"متن پردازش شده:\\n{result['processed_text']}\")\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "        print(f\"نتیجه تشخیص: {get_display(arabic_reshaper.reshape(result['prediction']))}\")\n",
        "        print(f\"احتمال جعلی بودن: {result['fake_probability']}\")\n",
        "        print(f\"احتمال واقعی بودن: {result['real_probability']}\")\n",
        "        print(\"=\"*50 + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjm+j5qM8M5405hwMLsehP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}